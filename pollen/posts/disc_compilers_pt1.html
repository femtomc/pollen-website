<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <title>Thoughts on compilers, the first part</title>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js">
    </script>
    <script type="text/javascript">
    function toggle_div(id) {
        e = document.getElementById(id);
        if (e.style.display == 'block') {
            e.style.display = 'none';
        }
        else if (e.style.display == 'none') {
            e.style.display = 'block';
        }
    }
    </script>
    <link rel="stylesheet" type="text/css"
          media="all" href="../css/posts.css"/>
    <link rel="stylesheet" type="text/css"
          media="all" href="../css/fonts.css"/>
</head>
    <body>
    <div id="doc"><h1>Thoughts on compilers, the first part</h1><section id="block"><p>I have this rather strong opinion that the more you learn about compilers, the better programmer and computer scientist you become.</p><p>To me, compilers are the archetypically representation of what computation is all about: accept data in one representation, transform it to another representation. This is of course the typical description - but I think these words do little to convey the point of the whole thing. The point being: the first representation contains some hidden set of assumed semantics which are described by the state transitions of another system. The transformation makes those semantics explicit by transforming the data into a form which can be accepted as an initial configuration of the other system. If this target system is a hardware implementation of an ISA, physics takes over and (ideally) executes according to what you thought. In other areas, this may just be an intermediate step - a place from which further compilation can ensue!</p><p>I think of the “semantic preserving transformation” as the core of the process. You may agree or disagree with this characterization, but I think you might agree that this characterization enjoys certain aesthetic qualities (it’s sufficiently ambiguous not to be ugly). Imagining that we are compiling to hardware - we are, after all, programming devices which are governed fundamentally by the laws of physics. Of course, the system requires an initial configuration which it can understand. Your weird pseudocode is not like that - it’s a symbolic representation which you use to understand that low-level system. That fundamentally complex, weird world of currents which is not intuitive to our language-riddled mammalian brain. But we must find a way to map our internal semantics onto that world. I find it strangely comforting that we can construct tools to effectively reason like this. It feels very primal - it reminds me in a very visceral way that I’m a dumb monkey.</p><p>It is very natural to build a theory of understanding on top of the set of transformations which take our weird etchings to the physical device level. What is easier for humans to understand? What is harder? If the target is fixed (mostly, of course there are differences in ISAs and SoCs - but nothing very drastic to the point of this post), there is quite a lot of flexibility in designing the process “take what the programmer means and execute it”. What exactly does the programmer mean? I think this is the hardest question in computer science. We are not good at this. That’s why programming is not a natural skill. We must contort our thoughts into a restricted version with many rules - contracts with the compiler. Herein lies the frustration - when we get upset with the compiler, we say “no, I didn’t want that”. Well of course you didn’t! But you forgot that you can’t express yourself as you normally would. So we come up with ways to bring that level of expression closer to what we understand. We think about composition, we describe monads, we curse imperative programming. These activities are quite intense - and people tend to get very invested in them. After all, it is fundamentally about how a person thinks.</p><p>Compilers have existed throughout human history. We just didn’t know they were compilers. Any abstraction which eases the task of our primitive thinking box has the potential to be a front end for a compiler. Human organizations are a type of compiler. Large scale engineering efforts themselves are compilers. It’s irrepressibly difficult for human beings to understand and control complex systems. So we come up with ways to abstract from the complexity. We come up with a representation of the device semantics and we agree to weird contractual rules which we constrain our thinking with. We make mistakes, we introduce bugs, and sometimes lives are lost. This is a fundamental aspect of our reasoning - it’s as unavoidable as the construction of compilers in the first place.</p><p>I think what most excites me about this perspective is that we’re just beginning to understand it. Formal representation of compilers is very new in human history. We’ve just begun to explore implementations, we’ve just begun to explore languages (like: category theory) for describing these processes. I think as long as human beings must think, must reason in the face of complexity which is overwhelming, the study of programming languages and compilers will thrive. This study is uniquely human - it deals fundamentally with our limitations, with our mistakes. It also deals fundamentally with how we compress knowledge for future generations - what is the best way to transmit the processes by which we reason about complexity to future humanity. This study is also uniquely suited to tell us more about ourselves, about why we think the way we do. Common abstractions which are widely appreciated are highly powerful statements about human intuition and aesthetics.</p></section></div>
</body>